{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"COLUMNS\"]=72\n",
    "using Pkg; for p in (\"Knet\",\"Plots\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using Base.Iterators: flatten\n",
    "using Statistics: mean\n",
    "using AutoGrad\n",
    "import .Iterators: cycle, Cycle, take, repeat\n",
    "using Plots; default(fmt=:png,ls=:auto)\n",
    "import Base: length, size, iterate, eltype, IteratorSize, IteratorEltype, haslength, @propagate_inbounds, repeat, rand, tail\n",
    "import .Iterators: cycle, Cycle, take, repeat\n",
    "using Knet: Knet, conv4, pool, adam!, mat, KnetArray, nll, zeroone, progress, sgd, param, param0, dropout, relu, Data, accuracy, progress!;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "dtrn, dtst = mnistdata(batchsize=100)\n",
    "\n",
    "#uncomment if you have a gpu\n",
    "#dtrn, dtst = mnistdata(batchsize=100, xtype=Knet.KnetArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv + Pool Layer\n",
    "struct ConvLayer; w; b; f; p; end\n",
    "(c::ConvLayer)(x) = c.f.(pool(conv4(c.w, dropout(x,c.p)) .+ c.b))\n",
    "ConvLayer(w1::Int, w2::Int, cx::Int, cy::Int, f=relu;pdrop=0)= ConvLayer(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv Layer\n",
    "struct Conv4Layer; w; b; f; end\n",
    "(c::Conv4Layer)(x) = c.f.(conv4(c.w) .+ c.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense Layer (Linear)\n",
    "struct DenseLayer; w; b; f; p; end\n",
    "(d::DenseLayer)(x) = d.f.(d.w * mat(dropout(x, d.p)) .+ d.b)\n",
    "DenseLayer(i::Int,o::Int,f=relu;pdrop=0) = DenseLayer(param(o,i), param0(o), f, pdrop)\n",
    "\n",
    "#SoftmaxLayer\n",
    "struct SoftmaxLayer; w; b; end\n",
    "(s::SoftmaxLayer)(x) = s.w * x .+ s.b\n",
    "(s::SoftmaxLayer)(x,y) = nll(s(x),y)\n",
    "(s::SoftmaxLayer)(d::Data) = mean(s(x,y) for (x,y) in d)\n",
    "SoftmaxLayer(i::Int,o::Int) = SoftmaxLayer(param(o,i), param0(o))\n",
    "\n",
    "#Chain\n",
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = nll(c(x),y)\n",
    "(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tgraph (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mytrain!(c::Chain, dtrn, dtst, valid=10, max_iters=500, )\n",
    "    \n",
    "    function pusher(c::Chain,dtrn,dtst,trnloss,tstloss)\n",
    "        push!(trnloss, c(dtrn))\n",
    "        push!(tstloss, c(dtst))\n",
    "    end\n",
    "    \n",
    "    trnloss = []\n",
    "    tstloss = []\n",
    "    \n",
    "    takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)       \n",
    "    #change the optimizer here: sgd, adam, ... @doc Knet.sgd to see other options :\n",
    "    a = sgd(c, take(cycle(dtrn), max_iters+1))\n",
    "    b = (pusher(c,dtrn,dtst,trnloss,tstloss) for x in takeevery(valid, a))\n",
    "    progress!(b)    \n",
    "    return 0:valid:max_iters, trnloss, tstloss\n",
    "end\n",
    "            \n",
    "function tgraph(c::Chain, dtrn, dtst; valid=10, max_iters=500)\n",
    "    Training_Accuracy = accuracy(c, dtrn)\n",
    "    Test_Accuracy = accuracy(c, dtst)\n",
    "    println(\"Training Accuracy: \", accuracy(c, dtrn))\n",
    "    println(\"Test Accuracy: \", accuracy(c, dtst))\n",
    "    iters, trnloss, tstloss = mytrain!(c,dtrn,dtst,valid,max_iters)\n",
    "    \n",
    "    println(\"Training Accuracy: \", accuracy(c, dtrn))\n",
    "    println(\"Test Accuracy: \", accuracy(c, dtst))\n",
    "    plot(iters, [trnloss, tstloss], labels=[:trn, :tst], xlabel=\"iterations\", ylabel=\"loss\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Various models to try out\n",
    "simple_model = Chain(ConvLayer(5,5,1,30),\n",
    "           DenseLayer(4320,10))\n",
    "\n",
    "lenet = Chain(ConvLayer(5,5,1,20), \n",
    "              ConvLayer(5,5,20,50),\n",
    "              DenseLayer(800, 500, pdrop=0.3),\n",
    "              DenseLayer(500,10,identity,pdrop=0.3))\n",
    "\n",
    "simple_model2 = Chain(ConvLayer(3,3,1,30),\n",
    "               ConvLayer(3,3,30,50),\n",
    "               DenseLayer(1250,700),\n",
    "               DenseLayer(700,400, pdrop=0),\n",
    "               DenseLayer(400,100, pdrop=0),\n",
    "               DenseLayer(100,10,pdrop=0,identity));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to train the model and graph loss functions, don't even try if you don't have a gpu \n",
    "#tgraph(simple_model, dtrn, dtst, max_iters = 1000)\n",
    "#tgraph(lenet, dtrn, dtst, max_iters = 1000)\n",
    "#tgraph(simple_model, dtrn, dtst, max_iters = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3 (4 threads)",
   "language": "julia",
   "name": "julia-1.0k"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
