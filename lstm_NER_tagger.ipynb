{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Named Entity Tagger\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Most research on NER systems has been structured as taking an unannotated block of text, such as the following **example**:\n",
    "\n",
    "**INPUT:** Jim bought 300 shares of Acme Corp. in 2006.\n",
    "\n",
    "And producing an annotated block of text that highlights the names of entities:\n",
    "\n",
    "**OUTPUT:** [Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\n",
    "\n",
    "In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.(Wikipedia)\n",
    "\n",
    "Your task in this lab is to implement named entity LSTM based tagger which uses an LSTM to extract features from the input sentence, which are then passed through a multi-layer perceptron to predict\n",
    "the tag of the word. Finally, train that model on [WikiNER](https://github.com/neulab/dynet-benchmark/tree/master/data/tags) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `Pkg.dir(pkgname, paths...)` is deprecated; instead, do `import Knet; joinpath(dirname(pathof(Knet)), \"..\", paths...)`.\n",
      "└ @ Pkg.API /opt/julia-1.0.3/share/julia/stdlib/v1.0/Pkg/src/API.jl:395\n"
     ]
    }
   ],
   "source": [
    "# Set-Up related files and Hyper-parameters\n",
    "using Pkg; for p in [\"Knet\",\"ArgParse\"]; haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using Knet\n",
    "import Knet: train!\n",
    "using Printf, Dates, Random\n",
    "STDOUT = Base.stdout\n",
    "using ArgParse\n",
    "import Base: length, iterate\n",
    "include(Pkg.dir(\"Knet\",\"data\",\"wikiner.jl\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. Prepare samples for the network\n",
    "Your first task is to prepare instances for the network. We're given with the tokens (words and tags) and we need to make them understandable by our neural network. For this purpose, we build vocabularies (for both words and tags) and construct vocabulary to index dictionaries by using those vocabularies (w2i and t2i, word2index, tag2index). Then, we convert words and tags to indices with the usage of our dictionaries. <br/>\n",
    "```julia\n",
    "julia> show_instance() # show instance in not implemented in Knet, it is a hypothetical procedure\n",
    "Inputs sentence:\n",
    "Sent-> That inscribed in the genealogical records of his family is Jiang Zhoutai .\n",
    "NERs-> O    O         O  O   O            O       O  O   O      O  I-PER I-PER   O\n",
    "\n",
    "Timesteps:\n",
    "Time step 1 ---> Inputs: That\n",
    "                 Outputs: O\n",
    "Time step 2 ---> Inputs: inscribed\n",
    "                 Outputs:O\n",
    "Time step 3 ---> Inputs: in\n",
    "                 Outputs: O\n",
    "Time step 4 ---> Inputs: the\n",
    "                 Outputs: O\n",
    "Time step 5 ---> Inputs: genealogical\n",
    "                 Outputs: O\n",
    "Time step 6 ---> Inputs: records  . \n",
    "                 Outputs: O\n",
    "Time step 7 ---> Inputs: of \n",
    "                 Outputs: O\n",
    "Time step 8 ---> Inputs: his \n",
    "                 Outputs: O\n",
    "Time step 9 ---> Inputs: family \n",
    "                 Outputs: O\n",
    "Time step 10 --->Inputs: is \n",
    "                 Outputs: O\n",
    "Time step 11 ---> Inputs: Jiang \n",
    "                  Outputs: I-PER\n",
    "Time step 12 ---> Inputs: Zhoutai \n",
    "                  Outputs: I-PER\n",
    "Time step 13 ---> Inputs: . \n",
    "                  Outputs: O\n",
    "```\n",
    "\n",
    "Our input and output arrays should be integers instead of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = WikiNERData();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.trn[1] = Array{SubString{String},1}[[\"The\", \"I-MISC\"], [\"Oxford\", \"I-MISC\"], [\"Companion\", \"I-MISC\"], [\"to\", \"I-MISC\"], [\"Philosophy\", \"I-MISC\"], [\"says\", \"O\"], [\",\", \"O\"], [\"\\\"\", \"O\"], [\"there\", \"O\"], [\"is\", \"O\"], [\"no\", \"O\"], [\"single\", \"O\"], [\"defining\", \"O\"], [\"position\", \"O\"], [\"that\", \"O\"], [\"all\", \"O\"], [\"anarchists\", \"O\"], [\"hold\", \"O\"], [\",\", \"O\"], [\"and\", \"O\"], [\"those\", \"O\"], [\"considered\", \"O\"], [\"anarchists\", \"O\"], [\"at\", \"O\"], [\"best\", \"O\"], [\"share\", \"O\"], [\"a\", \"O\"], [\"certain\", \"O\"], [\"family\", \"O\"], [\"resemblance\", \"O\"], [\".\", \"O\"], [\"\\\"\", \"O\"]]\n",
      "(data.trn[1])[1] = SubString{String}[\"The\", \"I-MISC\"]\n",
      "((data.trn[1])[1])[1] = \"The\"\n",
      "((data.trn[1])[1])[2] = \"I-MISC\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I-MISC\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data of type WikiNERData has 3 fields:\n",
    "\n",
    "data.w2i #dictionary with 28484 entries: word -> index\n",
    "data.t2i #dictionary with 9     entries: tag -> index\n",
    "data.trn #array with 142153 entries\n",
    "@show data.trn[1]       #array of 32 tuples (word, tag)\n",
    "@show data.trn[1][1]    #tuple\n",
    "@show data.trn[1][1][1] #word\n",
    "@show data.trn[1][1][2] #tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_instance (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make_instances procedure is given to you\n",
    "function make_instances(data, w2i, t2i)\n",
    "    words = []; tags = []\n",
    "    for k = 1:length(data)\n",
    "        this_words, this_tags = make_instance(data[k], w2i, t2i)\n",
    "        push!(words, this_words)\n",
    "        push!(tags, this_tags)\n",
    "    end\n",
    "    order = sortperm(words, by=length, rev=true)\n",
    "    return words, tags\n",
    "end\n",
    "\n",
    "#=\n",
    "You need to implement make_instance function\n",
    "instance is a list of tuples. Each tuple contains a word and the corresponding tag as string.\n",
    "You need to convert them into indices using word to index (w2i) and tag to index (t2i)\n",
    "=#\n",
    "function make_instance(instance, w2i, t2i)\n",
    "    input = []\n",
    "    output = [] \n",
    "    \n",
    "    # START ANSWER\n",
    "    for i in instance\n",
    "        key, tag = i\n",
    "        if !haskey(data.w2i, key)\n",
    "            push!(input, w2i[UNK])\n",
    "            push!(output, t2i[tag])\n",
    "        else\n",
    "            push!(input, w2i[key])\n",
    "            push!(output, t2i[tag])\n",
    "        end\n",
    "    end\n",
    "    # END ANSWER\n",
    "    return input, output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length (generic function with 129 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This struct contains processed data (e.g words and tags are indices)\n",
    "and necessary variables to prepare minibatches.\n",
    "WikiNERProcessed struct works as an iterator.\n",
    "=#\n",
    "mutable struct WikiNERProcessed\n",
    "    words\n",
    "    tags\n",
    "    batchsize\n",
    "    ninstances\n",
    "    shuffled\n",
    "end\n",
    "\n",
    "\n",
    "function WikiNERProcessed(instances, w2i, t2i; batchsize=16, shuffled=false)\n",
    "    words, tags = make_instances(instances, w2i, t2i)\n",
    "    ninstances = length(words)\n",
    "    return WikiNERProcessed(words, tags, batchsize, ninstances, shuffled)\n",
    "end\n",
    "\n",
    "\n",
    "function length(d::WikiNERProcessed)\n",
    "    d, r = divrem(d.ninstances, d.batchsize)\n",
    "    return r == 0 ? d : d+1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iterate (generic function with 251 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "You will use the RNN callable object in your model. It supports variable length instances in its input.\n",
    "However, you need to prepare your input such as the RNN object can work on it. See the batchSizes option of the RNN object.\n",
    "=#\n",
    "function iterate(d::WikiNERProcessed, state=ifelse(d.shuffled, randperm(d.ninstances), 1:d.ninstances))    \n",
    "    words = []\n",
    "    tags = []\n",
    "    batchsizes = zeros(d.batchsize)\n",
    "    \n",
    "    if state == \"stop\"\n",
    "        return nothing;\n",
    "    end\n",
    "        \n",
    "\n",
    "    if(first(state)+d.batchsize >= d.ninstances)\n",
    "        return ((words, tags, batchsizes), \"stop\")\n",
    "    end\n",
    "\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    for i in 1:50\n",
    "        push!(words, [])\n",
    "        push!(tags, [])\n",
    "    end\n",
    "    \n",
    "    batchsizes = zeros(50)\n",
    "    \n",
    "    \n",
    "    sentences = d.words[first(state):first(state)+d.batchsize-1]\n",
    "    labels = d.tags[first(state):first(state)+d.batchsize-1]\n",
    "       \n",
    "    for inarray in 1:50\n",
    "        for sentence_num in 1:length(sentences)\n",
    "            if length(sentences[sentence_num]) >= inarray\n",
    "                theword = sentences[sentence_num][inarray]\n",
    "                thetag = labels[sentence_num][inarray]\n",
    "                push!(words[inarray], theword)\n",
    "                push!(tags[inarray], thetag)\n",
    "                batchsizes[inarray] += 1  \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    new_state = (first(state)+d.batchsize):(last(state))\n",
    "    batchsizes = convert(KnetArray, batchsizes)\n",
    "    \n",
    "    return ((words, tags, batchsizes), new_state)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Implement Layers\n",
    "You need to implement layers that we are going to use in our tagger network. We supplied the layer definitions, but you still need to implement their initialization schemes and forward propagation functionality. Here, you need to implement three different layers,\n",
    "\n",
    "1. Embedding layer (projection of one hot column vectors, or just array indexing)\n",
    "2. Linear layer (just projection)\n",
    "3. Hidden layer (projection and then non-linear activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH CELL, take advantage of _usegpu and _atype in the following parts\n",
    "_usegpu = gpu()>=0\n",
    "_atype = ifelse(_usegpu, KnetArray{Float32}, Array{Float64})\n",
    "\n",
    "mutable struct Embedding\n",
    "    w # weight\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Linear\n",
    "    w # weight\n",
    "    b # bias\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Hidden\n",
    "    w # weight\n",
    "    b # bias\n",
    "    fun # non-linear activation function like relu or tanh\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hidden"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializations\n",
    "function Embedding(vocabsize::Int, embedsize::Int, atype=_atype, scale=0.01)\n",
    "    w = Param(convert(atype, scale*randn(embedsize, vocabsize)));\n",
    "    #w = Param(scale*randn(embedsize, vocabsize))\n",
    "    return Embedding(w)\n",
    "end\n",
    "\n",
    "\n",
    "function Linear(xsize::Int, ysize::Int, atype=_atype, scale=0.01)\n",
    "    # start your answer\n",
    "    o = xsize\n",
    "    i = ysize\n",
    "    Linear(scale * param(o,i), param0(o))\n",
    "    # end your answer\n",
    "end\n",
    "\n",
    "\n",
    "function Hidden(xsize::Int, ysize::Int, fun=relu, atype=_atype, scale=0.1)\n",
    "    # start your answer\n",
    "    i = xsize\n",
    "    o = ysize\n",
    "    Hidden(scale * param(o,i), param0(o), fun)\n",
    "    # end your answer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (l::Embedding)(x)\n",
    "    embedz = []\n",
    "    for i in x\n",
    "        temp = l.w[:, convert(Array{Int32,1}, i)]\n",
    "        @show size(i)\n",
    "        @show size(temp)\n",
    "        push!(embedz, temp)\n",
    "    end\n",
    "    embedz\n",
    "    return KnetArray(embedz)\n",
    "end\n",
    "\n",
    "(l::Linear)(x) = l.w * reshape(x, size(x)[1], size(x)[2]*size(x)[3]) .+ l.b\n",
    "\n",
    "(d::Hidden)(x) = d.f.(d.w * reshape(x, size(x)[1], size(x)[2]*size(x)[3]) .+ d.b);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "KnetPtr: bad device id -1.",
     "output_type": "error",
     "traceback": [
      "KnetPtr: bad device id -1.",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] Knet.KnetPtr(::Int64) at /mnt/juliabox/.julia/packages/Knet/3lzCR/src/kptr.jl:94",
      " [3] Type at /mnt/juliabox/.julia/packages/Knet/3lzCR/src/karray.jl:111 [inlined]",
      " [4] convert(::Type{KnetArray{Float64,1}}, ::Array{Float64,1}) at /mnt/juliabox/.julia/packages/Knet/3lzCR/src/karray.jl:150",
      " [5] convert(::Type{KnetArray}, ::Array{Float64,1}) at /mnt/juliabox/.julia/packages/Knet/3lzCR/src/karray.jl:148",
      " [6] iterate(::WikiNERProcessed, ::UnitRange{Int64}) at ./In[6]:46",
      " [7] iterate(::WikiNERProcessed) at ./In[6]:6",
      " [8] first(::WikiNERProcessed) at ./abstractarray.jl:288",
      " [9] top-level scope at In[17]:2"
     ]
    }
   ],
   "source": [
    "w1 = WikiNERProcessed(data.trn, data.w2i, data.t2i);\n",
    "words1, tags1, batchsizes1 = first(w1)\n",
    "E1 = Embedding(VOCABSIZE, EMBEDSIZE) \n",
    "embeddings1 = E1(words1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABSIZE = length(data.w2i)\n",
    "EMBEDSIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3. Implement Model\n",
    "You need to implement initweights function which takes input, hidden and output dimensions and returns the whole model as `Array{Any}` julia data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CELL\n",
    "mutable struct NERTagger\n",
    "    embed_layer::Embedding \n",
    "    rnn::RNN\n",
    "    hidden_layer::Hidden\n",
    "    softmax_layer::Linear\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERTagger"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model initialization\n",
    "# Check the array type (cpu vs gpu)\n",
    "# Initialize your modules using given arguments\n",
    "function NERTagger(rnn_hidden, words, tags, embed, mlp_hidden, usegpu=_usegpu, winit=0.01)\n",
    "    # start your answer\n",
    "    embed_layer = Embedding(VOCABSIZE, EMBEDSIZE)\n",
    "    rnn = RNN(EMBEDSIZE, rnn_hidden)\n",
    "    hidden_layer = Hidden(rnn_hidden, mlp_hidden)\n",
    "    softmax_layer = Linear(mlp_hidden, tags)\n",
    "    return NERTagger(embed_layer, rnn, hidden_layer, softmax_layer)\n",
    "    # end your answer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model forward propagation\n",
    "# Call your modules as described in the introduction section\n",
    "function (m::NERTagger)(x, batchsizes)\n",
    "    #start your answer\n",
    "    embed_out = m.embed_layer(x)\n",
    "    rnn_out = m.rnn(embed_out, batchSizes = batchsizes)\n",
    "    hidden_layer = m.hidden_layer(rnn_out)\n",
    "    softmax_out = m.softmax_layer(hidden_layer)\n",
    "    #end your answer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERTagger(Embedding(P(Array{Float64,2}(20,28484))), LSTM(input=20,hidden=50), Hidden([-0.0128861 0.0147508 … -0.00173876 -0.00817138; -0.0136994 0.00480354 … 0.00407628 -0.00563847; … ; 0.000620629 -0.00741351 … -0.0119897 -0.0102568; 0.00640071 0.00320762 … -0.00961443 -0.00129682], P(Array{Float32,1}(40)), Knet.relu), Linear([0.00157205 -0.00123316 … -0.00149301 -0.000248446; 0.00191285 0.000966202 … -0.00079508 0.000473948; … ; 0.000830567 1.64982e-6 … -0.00131028 0.00150225; -0.00146644 0.000483051 … 0.000237693 0.00154957], P(Array{Float32,1}(40))))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMRNN = 50\n",
    "NUMWORDS = length(data.w2i)\n",
    "NUMTAGS = length(data.t2i)\n",
    "NUMMLP = 40\n",
    "EMBEDSIZE = 20\n",
    "\n",
    "model = NERTagger(NUMRNN, NUMWORDS, NUMTAGS, EMBEDSIZE, NUMMLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4. Implement Loss Function\n",
    "Implement loss functions defined the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your probabilities from your model \n",
    "# Calculate the loss function for average per token.\n",
    "function (m::NERTagger)(x, batchsizes, ygold)\n",
    "    # start your answer\n",
    "    softmax_out = model(x, batchsizes)\n",
    "    ypredict = predict(m, x, batchsizes)\n",
    "    Knet.nll(ypredict, ygold)\n",
    "    # end your answer\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5. Implement Accuracy function\n",
    "Accuracy function counts the number of words(tokens) and also counts the number of correctly predicted tokens and returns ```number_of_correctly_pred_token / number_of_total_token```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possible helpful procedures: argmax, vec\n",
    "function accuracy(m::NERTagger, data, i2t)\n",
    "    ncorrect = 0\n",
    "    ntokens = 0\n",
    "    for (x, ygold, batchsizes) in data\n",
    "        ypredict = predict(m,x,batchsizes)\n",
    "        for i in 1:length(ypredict)\n",
    "            if ygold[i] == ypredict[i]\n",
    "                ncorrect += 1\n",
    "            end\n",
    "            ntokens+=1\n",
    "        end\n",
    "    end\n",
    "    return ncorrect/ntokens\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T TOUCH this cell\n",
    "function main(args)\n",
    "    o = parse_options(args)\n",
    "    atype = o[:atype]\n",
    "    display(o)\n",
    "    o[:seed] > 0 && Knet.seed!(o[:seed])\n",
    "\n",
    "    # load WikiNER data\n",
    "    data = WikiNERData()\n",
    "    \n",
    "    # build model\n",
    "    nwords, ntags = length(data.w2i), data.ntags\n",
    "    model = NERTagger(o[:hidden], nwords, ntags, o[:embed], o[:mlp])\n",
    "    initopt!(model)\n",
    "    # opt = optimizers(w, Adam)\n",
    "\n",
    "    # make instances\n",
    "    trn = WikiNERProcessed(data.trn, data.w2i, data.t2i; batchsize=o[:batchsize])\n",
    "    dev = WikiNERProcessed(data.dev, data.w2i, data.t2i; batchsize=o[:batchsize])\n",
    "\n",
    "    # train bilstm tagger\n",
    "    nwords = data.nwords\n",
    "    ninstances = length(trn)\n",
    "    println(\"nwords=$nwords, ntags=$ntags, ninstances=$ninstances\"); flush(STDOUT)\n",
    "    println(\"startup time: \", Int((now()-t00).value)*0.001); flush(STDOUT)\n",
    "    t0 = now()\n",
    "    all_time = dev_time = all_tagged = this_tagged = this_loss = 0\n",
    "    iter = 0\n",
    "    while true\n",
    "        # training\n",
    "        for (k, (x, ygold, batchsizes)) in enumerate(trn)\n",
    "            num_tokens = length(x)\n",
    "            # instance_loss = adam!(model, (x, ygold, batchsizes))\n",
    "            instance_loss = mytrain!(model, x, batchsizes, ygold)\n",
    "            this_loss += num_tokens*instance_loss\n",
    "            this_tagged += num_tokens\n",
    "            iter += 1\n",
    "            if iter % o[:report] == 0\n",
    "                println(this_loss/this_tagged); flush(STDOUT)\n",
    "            end\n",
    "            if iter % o[:valid] == 0\n",
    "                # validation\n",
    "                dev_start = now()\n",
    "                tag_acc = accuracy(model, dev, data.i2t)\n",
    "                dev_time += Int((now()-dev_start).value)*0.001\n",
    "                train_time = Int((now()-t0).value)*0.001-dev_time\n",
    "\n",
    "                # report\n",
    "                @printf(\"%d iters finished, loss=%f\\n\", iter, this_loss/this_tagged)\n",
    "                all_tagged += this_tagged\n",
    "                this_loss = this_tagged = 0\n",
    "                all_time = Int((now()-t0).value)*0.001\n",
    "                @printf(\"tag_acc=%.4f, time=%.4f, word_per_sec=%.4f\\n\",\n",
    "                    tag_acc, train_time, all_tagged/train_time)\n",
    "                flush(STDOUT)\n",
    "            end\n",
    "            iter >= o[:iters] && return\n",
    "        end\n",
    "\n",
    "    end\n",
    "end\n",
    "\n",
    "function parse_options(args)\n",
    "    s = ArgParseSettings()\n",
    "    s.description = \"LSTM Tagger in Knet\"\n",
    "\n",
    "    @add_arg_table s begin\n",
    "        (\"--usegpu\"; action=:store_true; help=\"use GPU or not\")\n",
    "        (\"--embed\"; arg_type=Int; default=128; help=\"word embedding size\")\n",
    "        (\"--hidden\"; arg_type=Int; default=50; help=\"LSTM hidden size\")\n",
    "        (\"--mlp\"; arg_type=Int; default=32; help=\"MLP size\")\n",
    "        (\"--epochs\"; arg_type=Int; default=3; help=\"number of training epochs\")\n",
    "        (\"--iters\"; arg_type=Int; default=20000; help=\"number of training iterations\")\n",
    "        (\"--report\"; arg_type=Int; default=500; help=\"report period in iters\")\n",
    "        (\"--valid\"; arg_type=Int; default=5000; help=\"valid period in iters\")\n",
    "        (\"--seed\"; arg_type=Int; default=-1; help=\"random seed\")\n",
    "        (\"--batchsize\"; arg_type=Int; default=16; help=\"batchsize\")\n",
    "    end\n",
    "\n",
    "    isa(args, AbstractString) && (args=split(args))\n",
    "    o = parse_args(args, s; as_symbols=true)\n",
    "    o[:atype] = (gpu() >= 0 && o[:usegpu]) ? KnetArray{Float32} : Array{Float64}\n",
    "    println(o); flush(STDOUT)\n",
    "    return o\n",
    "end\n",
    "\n",
    "\n",
    "function mytrain!(model::NERTagger, x, batchsizes, ygold)\n",
    "    values = []\n",
    "    J = @diff model(x, batchsizes, ygold)\n",
    "    for par in params(model)\n",
    "        g = grad(J, par)\n",
    "        update!(value(par), g, par.opt)\n",
    "    end\n",
    "    return value(J)\n",
    "end\n",
    "\n",
    "function initopt!(model::NERTagger, optimizer=\"Adam()\")\n",
    "    for par in params(model)\n",
    "        par.opt = eval(Meta.parse(optimizer))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t00 = now();main(\"--seed 1 --iters 10000 --usegpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
