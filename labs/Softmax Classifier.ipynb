{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying MNIST digits with a softmax classifier\n",
    "#Name: Ege ErsÃ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a softmax classifier to predict the digit presented in a given image. We will use the MNIST dataset for this task. Please first skim through the notebook. Then complete the following steps mentioned in the main function:\n",
    "\n",
    "1. minibatch\n",
    "2. init_params\n",
    "3. forward and backward propagation\n",
    "    * softmax_forw\n",
    "    * softmax_cost\n",
    "4. grad_check\n",
    "5. train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; for p in [\"Knet\"]; haskey(Pkg.installed(),p) || Pkg.add(p); end #Knet installation to use the MNIST dataset\n",
    "using Knet, Printf, Random\n",
    "\n",
    "include(Knet.dir(\"data\", \"mnist.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch2 (generic function with 2 methods)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " function minibatch2(X, Y, bs=100)\n",
    "    #takes raw input (X) and gold labels (Y)\n",
    "    #returns list of minibatches (x, y)\n",
    "    data = Any[]\n",
    "    N = size(Y)[2]\n",
    "    @show N\n",
    "    for i in 1:bs:N\n",
    "        if i+bs-1 >= N\n",
    "            batchX = X[:,i:N]\n",
    "            batchY = Y[:,i:N]\n",
    "            push!(data, (batchX, batchY))\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        batchX = X[:,i:i+bs-1]\n",
    "        batchY = Y[:,i:i+bs-1]\n",
    "        #@show batchX, batchY\n",
    "        push!(data, (batchX, batchY))\n",
    "    end\n",
    "    return data                                                                 \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#here is a little toy example to see what a minibatch looks like\n",
    "\n",
    "#take first 6 data points only\n",
    "little_xtrn = xtrn[:,1:10]\n",
    "little_ytrn = ytrn[:,1:10]\n",
    "\n",
    "#call the minibatch function, it will give you a list of batches (batches are tuples)\n",
    "mini_batch1 = minibatch2(little_xtrn, little_ytrn, 4)\n",
    "\n",
    "#take the first batch (a tuple) from the list of batches\n",
    "batch1 = mini_batch1[1]\n",
    "\n",
    "#you can simply get X and Y components of the batch from the tuple\n",
    "batch1_X = batch1[1]\n",
    "batch1_Y = batch1[2]\n",
    "\n",
    "#once you have the batch, you can get a single y vector from the array if you want\n",
    "Y1 = batch1_Y[:,1]\n",
    "\n",
    "\n",
    "@show mini_batch1[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_params (generic function with 1 method)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_params(ninputs, noutputs)\n",
    "    #takes number of inputs and number of outputs(number of classes)\n",
    "    #returns randomly generated W and b(must be zeros vector) params of softmax\n",
    "    \n",
    "    #start of step 2\n",
    "    W = 0.001 * randn(noutputs, ninputs)\n",
    "    b = zeros(Float64, noutputs)\n",
    "    W, b\n",
    "    #end of step 2                                                              \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_forw (generic function with 1 method)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax_forw(W, b, data)\n",
    "    #applies the affine transformation and softmax function\n",
    "    #returns predicted probabilities\n",
    "    \n",
    "    ### step 3_1\n",
    "    output = broadcast(+, (W * data), b)\n",
    "    output = (x->MathConstants.e^x).(output)\n",
    "    sums = sum(output, dims=1)\n",
    "    output = broadcast(/, output, sums)\n",
    "    ### step 3_1                                                                  \n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff, AutoGrad, Knet\n",
    "\n",
    "function softmax_cost(W, b, data, labels)\n",
    "    #takes W, b paremeters, data and correct labels\n",
    "    #calculates the soft loss, gradient of w and gradient of b\n",
    "    #data is a batch of Xs (784, 100)\n",
    "    #labels is a batch of Ys (10, 100)\n",
    "    batch_size = size(labels)[2]\n",
    "    \n",
    "    #get predictions as a probability distribution for each xi\n",
    "    prediction = softmax_forw(W, b, data)\n",
    "    \n",
    "    #indexes of correct labels\n",
    "    correct_labels = argmax(labels, dims=1)\n",
    "\n",
    "    #get the prob corresponding to correct label\n",
    "    correct_probs = prediction[correct_labels]\n",
    "    \n",
    "    #sum the -logs of individual prob values\n",
    "    loss = sum((x->-log(x)).(correct_probs))/batch_size\n",
    "    \n",
    "    dscores = prediction\n",
    "    for n in 1:batch_size\n",
    "        yi = argmax(labels[:,n])\n",
    "        dscores[yi,n] -= 1\n",
    "    end\n",
    "    dscores /= batch_size\n",
    "    dw = (data * dscores')'\n",
    "    db = sum(dscores, dims=2)\n",
    "    loss, dw, db\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 1.8315191059140225e-9\n",
      "Gradient Checking Passed\n"
     ]
    }
   ],
   "source": [
    "function grad_check(W, b, data, labels)\n",
    "    function numeric_loss(W, b, data, labels)\n",
    "        batch_size = size(labels)[2]\n",
    "        prediction = softmax_forw(W, b, data)\n",
    "        correct_labels = argmax(labels, dims=1)\n",
    "        correct_probs = prediction[correct_labels]\n",
    "        loss = sum((x->-log(x)).(correct_probs))/batch_size\n",
    "    end\n",
    "    \n",
    "    \n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.0001\n",
    "        \n",
    "        gw = zeros(size(W))\n",
    "        gb = zeros(size(b))\n",
    "        \n",
    "        #start of step 4\n",
    "        for i in 1:10\n",
    "            for j in 1:784\n",
    "                W[i,j] += epsilon\n",
    "                L1 = numeric_loss(W, b, data, labels)\n",
    "                W[i,j] -= 2*epsilon\n",
    "                L2 = numeric_loss(W, b, data, labels)    \n",
    "                gw[i,j] = (L1-L2)/(2*epsilon)\n",
    "                W[i,j] += epsilon\n",
    "            end\n",
    "        end               \n",
    "        \n",
    "        for i in 1:size(b)[1]\n",
    "            b[i] += epsilon\n",
    "            L1 = numeric_loss(W,b,data,labels)\n",
    "            b[i] -= 2*epsilon\n",
    "            L2 = numeric_loss(W,b,data,labels)\n",
    "            gb[i] = (L1-L2)/(2*epsilon)\n",
    "            b[i] += epsilon\n",
    "        end\n",
    "        #end of step 4\n",
    "        return gw, gb\n",
    "    end\n",
    "    \n",
    "    _,gradW,gradB = softmax_cost(W, b, data, labels)\n",
    "    gw, gb = numeric_gradient()\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "    end                                                                         \n",
    "end\n",
    "\n",
    "debug = true #Turn this parameter off, after gradient checking passed\n",
    "if debug\n",
    "    grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STEP 5: Training\n",
    "#  The train function takes model parameters and the data\n",
    "#  Trains the model over minibatches\n",
    "#  For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "#  train function returns the average cost per instance\n",
    "    \n",
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        #start of step 5\n",
    "        #YOUR CODE HERE\n",
    "        batch_cost, batch_dw, batch_db = softmax_cost(W, b, x, y)\n",
    "        #@show batch_cost\n",
    "        totalcost += batch_cost\n",
    "        W -= lr * batch_dw \n",
    "        b -= lr * batch_db \n",
    "        numins += 1\n",
    "        #end of step 5\n",
    "    end\n",
    "\n",
    "    avgcost = totalcost / numins    \n",
    "    \n",
    "    return avgcost, W, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy2 (generic function with 1 method)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function accuracy2(ygold, yhat)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += findmax(ygold[:,i]; dims=1)[2] == findmax(yhat[:, i]; dims=1)[2] ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)                                             \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "    Random.seed!(12345)\n",
    "    \n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "    \n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "    \n",
    "    ## Data loading & preprocessing\n",
    "    #\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,1,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "    #  Size of ytrn must be: (10, 10000)\n",
    "    #  Size of ytst must be: (10, 10000)\n",
    "    \n",
    "    xtrn, ytrn, xtst, ytst = mnist() # loading the data\n",
    "    xtrn = reshape(xtrn, 784, 60000)\n",
    "    xtst = reshape(xtst, 784, 10000)\n",
    "    \n",
    "    function to_onehot(x)\n",
    "        onehot = zeros(10, 1)\n",
    "        onehot[x, 1] = 1.0\n",
    "        return onehot\n",
    "    end\n",
    "    \n",
    "    ytrn = hcat(map(to_onehot, ytrn)...)\n",
    "    ytst = hcat(map(to_onehot, ytst)...)\n",
    "    \n",
    "    ## STEP 1: Create minibatches\n",
    "    # Complete the minibatch function\n",
    "    # It takes the input matrix (X) and gold labels (Y)\n",
    "    # returns list of tuples contain minibatched input and labels (x, y)\n",
    "    bs = 100\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "    \n",
    "    ## STEP 2: Initialize parameters\n",
    "    #  Complete init_params function\n",
    "    #  It takes number of inputs and number of outputs(number of classes)\n",
    "    #  It returns randomly generated W matrix and bias vector\n",
    "    #  Sample from N(0, 0.001)\n",
    "    \n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "    \n",
    "    ## STEP 3: Implement softmax_forw and softmax_cost\n",
    "    #  softmax_forw function takes W, b, and data\n",
    "    #  calculates predicted probabilities\n",
    "    #\n",
    "    #  softmax_cost function obtains probabilites by calling softmax_forw\n",
    "    #  then calculates soft loss and\n",
    "    #  gradient of W and gradient of b\n",
    "    \n",
    "    ## STEP 4: Gradient checking\n",
    "    #  Skip this part for the lab session.\n",
    "    #  As with any learning algorithm, you should always check that your\n",
    "    #  gradients are correct before learning the parameters.\n",
    "    \n",
    "    debug = true #Turn this parameter off, after gradient checking passed\n",
    "    if debug\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "    \n",
    "    lr = 0.15\n",
    "    \n",
    "    ## STEP 5: Training\n",
    "    #  The train function takes model parameters and the data\n",
    "    #  Trains the model over minibatches\n",
    "    #  For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "    #  train function returns the average cost per instance\n",
    "    \n",
    "    for i=1:50   \n",
    "        cost, W, b = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy2(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy2(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 1.830923770917749e-9\n",
      "Gradient Checking Passed\n",
      "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
      "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
      "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
      "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
      "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
      "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
      "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
      "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
      "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
      "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
      "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
      "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
      "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
      "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
      "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
      "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
      "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
      "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
      "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
      "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
      "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
      "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
      "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
      "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
      "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
      "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
      "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
      "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
      "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
      "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
      "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
      "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
      "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
      "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
      "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
      "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
      "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
      "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
      "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
      "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
      "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
      "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
      "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
      "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
      "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
      "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
      "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
      "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
      "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
      "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#= Example Output\n",
    "Diff: 1.8292339049184216e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "=#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
